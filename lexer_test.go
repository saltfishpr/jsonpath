package jsonpath

import (
	"testing"
)

// TestLexerTokenTypes æµ‹è¯•æ‰€æœ‰åŸºæœ¬ token ç±»å‹çš„è¯†åˆ«
func TestLexerTokenTypes(t *testing.T) {
	tests := []struct {
		input    string
		expected TokenType
	}{
		// æ ¹èŠ‚ç‚¹å’Œå½“å‰èŠ‚ç‚¹æ ‡è¯†ç¬¦
		{"$", TokenRoot},
		{"@", TokenCurrent},

		// è¿ç®—ç¬¦
		{".", TokenDot},
		{"..", TokenDotDot},
		{"[", TokenLBracket},
		{"]", TokenRBracket},
		{",", TokenComma},
		{"?", TokenQuestion},
		{":", TokenColon},
		{"*", TokenWildcard},

		// æ¯”è¾ƒè¿ç®—ç¬¦
		{"==", TokenEq},
		{"!=", TokenNe},
		{"<", TokenLt},
		{"<=", TokenLe},
		{">", TokenGt},
		{">=", TokenGe},

		// é€»è¾‘è¿ç®—ç¬¦
		{"&&", TokenLAnd},
		{"||", TokenLOr},
		{"!", TokenLNot},

		// æ‹¬å·
		{"(", TokenLParen},
		{")", TokenRParen},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != tt.expected {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›ç±»å‹ %v, å®é™… %v", tt.input, tt.expected, token.Type)
			}
		})
	}
}

// TestLexerKeywords æµ‹è¯•å…³é”®å­—è¯†åˆ«
func TestLexerKeywords(t *testing.T) {
	tests := []struct {
		input    string
		expected TokenType
	}{
		{"true", TokenTrue},
		{"false", TokenFalse},
		{"null", TokenNull},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != tt.expected {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›ç±»å‹ %v, å®é™… %v", tt.input, tt.expected, token.Type)
			}
		})
	}
}

// TestLexerIdentifiers æµ‹è¯•æ ‡è¯†ç¬¦è¯†åˆ«
func TestLexerIdentifiers(t *testing.T) {
	tests := []struct {
		input string
		value string
	}{
		// å‡½æ•°å - RFC 9535 Section 2.4
		// function-name-first = LCALPHA (a-z)
		{"length", "length"},
		{"count", "count"},
		{"match", "match"},
		{"search", "search"},
		{"value", "value"},
		{"foo", "foo"},
		{"my_function", "my_function"},
		{"func123", "func123"},

		// æˆå‘˜åç®€å†™ - RFC 9535 Section 2.5.1
		// name-first = ALPHA / "_" / %x80-D7FF / %xE000-10FFFF
		{"name", "name"},
		{"_private", "_private"},
		{"camelCase", "camelCase"},
		{"PascalCase", "PascalCase"},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != TokenIdent {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›ç±»å‹ TokenIdent, å®é™… %v", tt.input, token.Type)
			}
			if token.Value != tt.value {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›å€¼ %q, å®é™… %q", tt.input, tt.value, token.Value)
			}
		})
	}
}

// TestLexerNumbers æµ‹è¯•æ•°å­—å­—é¢é‡
func TestLexerNumbers(t *testing.T) {
	tests := []struct {
		input       string
		expectType  TokenType
		expectValue string
	}{
		// RFC 9535 Section 2.3.3: int = "0" / (["-"] DIGIT1 *DIGIT)
		{"0", TokenNumber, "0"},
		{"1", TokenNumber, "1"},
		{"123", TokenNumber, "123"},
		{"-1", TokenNumber, "-1"},
		{"-123", TokenNumber, "-123"},

		// RFC 9535: -0 æ˜¯åˆæ³•çš„ç‰¹æ®Šæƒ…å†µ
		{"-0", TokenNumber, "-0"},

		// å¸¦å°æ•°éƒ¨åˆ†
		{"0.5", TokenNumber, "0.5"},
		{"3.14", TokenNumber, "3.14"},
		{"-2.5", TokenNumber, "-2.5"},

		// å¸¦æŒ‡æ•°éƒ¨åˆ†
		{"1e10", TokenNumber, "1e10"},
		{"1E10", TokenNumber, "1E10"},
		{"1e+10", TokenNumber, "1e+10"},
		{"1e-10", TokenNumber, "1e-10"},
		{"-1e10", TokenNumber, "-1e10"},

		// å¤æ‚æƒ…å†µ
		{"123.456e78", TokenNumber, "123.456e78"},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != tt.expectType {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›ç±»å‹ %v, å®é™… %v", tt.input, tt.expectType, token.Type)
			}
			if token.Value != tt.expectValue {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›å€¼ %q, å®é™… %q", tt.input, tt.expectValue, token.Value)
			}
		})
	}
}

// TestLexerInvalidNumbers æµ‹è¯•éæ³•æ•°å­—æ ¼å¼
func TestLexerInvalidNumbers(t *testing.T) {
	tests := []struct {
		input         string
		expectIllegal bool
	}{
		// RFC 9535 ç¦æ­¢å‰å¯¼é›¶ï¼ˆé™¤äº†å•ç‹¬çš„ 0ï¼‰
		{"01", true},
		{"-01", true},
		{"001", true},

		// å…¶ä»–éæ³•æ ¼å¼
		{"-", true},   // åªæœ‰è´Ÿå·
		{"1.", true},  // å°æ•°ç‚¹åæ²¡æœ‰æ•°å­—
		{"1e", true},  // æŒ‡æ•°åæ²¡æœ‰æ•°å­—
		{"1e+", true}, // æŒ‡æ•°ç¬¦å·åæ²¡æœ‰æ•°å­—
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			isIllegal := token.Type == TokenIllegal
			if isIllegal != tt.expectIllegal {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›éæ³•=%v, å®é™…=%v (tokenç±»å‹=%v)", tt.input, tt.expectIllegal, isIllegal, token.Type)
			}
		})
	}
}

// TestLexerStrings æµ‹è¯•å­—ç¬¦ä¸²å­—é¢é‡
func TestLexerStrings(t *testing.T) {
	tests := []struct {
		input       string
		expectType  TokenType
		expectValue string
	}{
		// åŒå¼•å·å­—ç¬¦ä¸²
		{`"hello"`, TokenString, "hello"},
		{`""`, TokenString, ""},

		// å•å¼•å·å­—ç¬¦ä¸² - RFC 9535 Section 2.3.1 æ”¯æŒ
		{`'hello'`, TokenString, "hello"},
		{`''`, TokenString, ""},

		// è½¬ä¹‰åºåˆ— - RFC 9535 Section 2.3.1 Table 4
		{`"\b"`, TokenString, "\b"}, // U+0008 BS backspace
		{`"\f"`, TokenString, "\f"}, // U+000C FF form feed
		{`"\n"`, TokenString, "\n"}, // U+000A LF line feed
		{`"\r"`, TokenString, "\r"}, // U+000D CR carriage return
		{`"\t"`, TokenString, "\t"}, // U+0009 HT horizontal tab
		{`"\/"`, TokenString, "/"},  // U+002F slash
		{`"\\"`, TokenString, "\\"}, // U+005C backslash
		{`"\""`, TokenString, `"`},  // U+0022 quotation mark
		{`"\'"`, TokenString, "'"},  // U+0027 apostrophe
		{`'"'`, TokenString, `"`},   // åŒå¼•å·åœ¨å•å¼•å·å­—ç¬¦ä¸²ä¸­
		{`'\''`, TokenString, "'"},  // å•å¼•å·åœ¨å•å¼•å·å­—ç¬¦ä¸²ä¸­éœ€è¦è½¬ä¹‰

		// Unicode è½¬ä¹‰
		{`"\u0041"`, TokenString, "A"},        // åŸºæœ¬å¤šæ–‡ç§å¹³é¢
		{`"\u4e2d\u6587"`, TokenString, "ä¸­æ–‡"}, // ä¸­æ–‡å­—ç¬¦

		// æ··åˆå†…å®¹
		{`"hello\nworld"`, TokenString, "hello\nworld"},
		{`"path\to\\file"`, TokenString, "path\to\\file"},

		// ç‰¹æ®Šå­—ç¬¦åœ¨å­—ç¬¦ä¸²ä¸­
		{`"a[b]c"`, TokenString, "a[b]c"},
		{`"name.with.dots"`, TokenString, "name.with.dots"},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != tt.expectType {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›ç±»å‹ %v, å®é™… %v", tt.input, tt.expectType, token.Type)
			}
			if token.Value != tt.expectValue {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›å€¼ %q, å®é™… %q", tt.input, tt.expectValue, token.Value)
			}
		})
	}
}

// TestLexerInvalidStrings æµ‹è¯•éæ³•å­—ç¬¦ä¸²æ ¼å¼
func TestLexerInvalidStrings(t *testing.T) {
	tests := []struct {
		input string
	}{
		{`"unclosed`}, // æœªé—­åˆçš„åŒå¼•å·å­—ç¬¦ä¸²
		{`'unclosed`}, // æœªé—­åˆçš„å•å¼•å·å­—ç¬¦ä¸²
		{`"\x"`},      // éæ³•è½¬ä¹‰åºåˆ—
		{`"\u"`},      // ä¸å®Œæ•´çš„ Unicode è½¬ä¹‰
		{`"\u123"`},   // Unicode è½¬ä¹‰åªæœ‰3ä½æ•°å­—
		{`"\u12G4"`},  // Unicode è½¬ä¹‰åŒ…å«éåå…­è¿›åˆ¶å­—ç¬¦
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != TokenIllegal {
				t.Errorf("è¾“å…¥ %q: æœŸæœ› TokenIllegal, å®é™… %v", tt.input, token.Type)
			}
		})
	}
}

// TestLexerSurrogatePairs æµ‹è¯• Unicode ä»£ç†å¯¹å¤„ç†
func TestLexerSurrogatePairs(t *testing.T) {
	tests := []struct {
		input       string
		expectType  TokenType
		expectValue string
	}{
		// RFC 9535 Section 2.3.1: ä»£ç†å¯¹å¤„ç†
		// é«˜ä»£ç†: D800-DBFF, ä½ä»£ç†: DC00-DFFF
		{`"\uD83D\uDE00"`, TokenString, "ğŸ˜€"}, // ç¬‘è„¸ emoji
		{`"\uD83C\uDC41"`, TokenString, "ğŸ"}, // DOMINO TILE
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != tt.expectType {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›ç±»å‹ %v, å®é™… %v", tt.input, tt.expectType, token.Type)
			}
			if token.Value != tt.expectValue {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›å€¼ %q (U+%04X), å®é™… %q (U+%04X)",
					tt.input, tt.expectValue, []rune(tt.expectValue)[0],
					token.Value, []rune(token.Value)[0])
			}
		})
	}
}

// TestLexerWhitespace æµ‹è¯•ç©ºç™½å­—ç¬¦å¤„ç†
func TestLexerWhitespace(t *testing.T) {
	// RFC 9535 Section 2.1.1: B = %x20 / %x09 / %x0A / %x0D
	// ç©ºæ ¼ / æ°´å¹³åˆ¶è¡¨ç¬¦ / æ¢è¡Œ / å›è½¦
	tests := []struct {
		input  string
		expect []TokenType
	}{
		{"$  [  ]", []TokenType{TokenRoot, TokenLBracket, TokenRBracket}},
		{"$\t[\n]", []TokenType{TokenRoot, TokenLBracket, TokenRBracket}},
		{"$\r\n[\r]", []TokenType{TokenRoot, TokenLBracket, TokenRBracket}},
		{"$  .  name  ", []TokenType{TokenRoot, TokenDot, TokenIdent}},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			for i, expectType := range tt.expect {
				token := lexer.NextToken()
				if token.Type != expectType {
					t.Errorf("ä½ç½® %d: æœŸæœ›ç±»å‹ %v, å®é™… %v", i, expectType, token.Type)
				}
			}
		})
	}
}

// TestLexerComplexExpressions æµ‹è¯•å¤æ‚çš„ JSONPath è¡¨è¾¾å¼
func TestLexerComplexExpressions(t *testing.T) {
	tests := []struct {
		input  string
		tokens []Token
	}{
		// RFC 9535 Figure 1 ç¤ºä¾‹
		{
			`$.store.book[0].title`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "store"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "book"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenNumber, Value: "0"},
				{Type: TokenRBracket, Value: "]"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "title"},
			},
		},
		// æ‹¬å·è¡¨ç¤ºæ³•
		{
			`$['store']['book'][0]['title']`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenString, Value: "store"},
				{Type: TokenRBracket, Value: "]"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenString, Value: "book"},
				{Type: TokenRBracket, Value: "]"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenNumber, Value: "0"},
				{Type: TokenRBracket, Value: "]"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenString, Value: "title"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// é€šé…ç¬¦
		{
			`$.store.book[*].author`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "store"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "book"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenWildcard, Value: "*"},
				{Type: TokenRBracket, Value: "]"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "author"},
			},
		},
		// åä»£æ®µ
		{
			`$..author`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenDotDot, Value: ".."},
				{Type: TokenIdent, Value: "author"},
			},
		},
		// æ•°ç»„åˆ‡ç‰‡
		{
			`$[0:10:2]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenNumber, Value: "0"},
				{Type: TokenColon, Value: ":"},
				{Type: TokenNumber, Value: "10"},
				{Type: TokenColon, Value: ":"},
				{Type: TokenNumber, Value: "2"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// è´Ÿç´¢å¼•
		{
			`$..book[-1]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenDotDot, Value: ".."},
				{Type: TokenIdent, Value: "book"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenNumber, Value: "-1"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// å¤šé€‰
		{
			`$..book[0,1]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenDotDot, Value: ".."},
				{Type: TokenIdent, Value: "book"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenNumber, Value: "0"},
				{Type: TokenComma, Value: ","},
				{Type: TokenNumber, Value: "1"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// è¿‡æ»¤å™¨è¡¨è¾¾å¼ - RFC 9535 Section 2.3.5
		{
			`$.store.book[?@.price < 10]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "store"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "book"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "price"},
				{Type: TokenLt, Value: "<"},
				{Type: TokenNumber, Value: "10"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// é€»è¾‘è¿ç®—ç¬¦
		{
			`$[?@.price < 10 && @.category == 'fiction']`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "price"},
				{Type: TokenLt, Value: "<"},
				{Type: TokenNumber, Value: "10"},
				{Type: TokenLAnd, Value: "&&"},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "category"},
				{Type: TokenEq, Value: "=="},
				{Type: TokenString, Value: "fiction"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// å‡½æ•°è°ƒç”¨ - RFC 9535 Section 2.4
		{
			`$[?length(@.authors) >= 5]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenIdent, Value: "length"},
				{Type: TokenLParen, Value: "("},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "authors"},
				{Type: TokenRParen, Value: ")"},
				{Type: TokenGe, Value: ">="},
				{Type: TokenNumber, Value: "5"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// å­˜åœ¨æ€§æµ‹è¯•
		{
			`$..book[?@.isbn]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenDotDot, Value: ".."},
				{Type: TokenIdent, Value: "book"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "isbn"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// å¸¦æ‹¬å·çš„è¡¨è¾¾å¼
		{
			`$[?(@.price < 10)]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenLParen, Value: "("},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "price"},
				{Type: TokenLt, Value: "<"},
				{Type: TokenNumber, Value: "10"},
				{Type: TokenRParen, Value: ")"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// é€»è¾‘é
		{
			`$[?!@.isbn]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenLNot, Value: "!"},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "isbn"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// å‡½æ•°è°ƒç”¨å¤šä¸ªå‚æ•°
		{
			`$[?match(@.date, "1974-05-..")]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenIdent, Value: "match"},
				{Type: TokenLParen, Value: "("},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "date"},
				{Type: TokenComma, Value: ","},
				{Type: TokenString, Value: "1974-05-.."},
				{Type: TokenRParen, Value: ")"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// é€†åºåˆ‡ç‰‡
		{
			`$[::-1]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenColon, Value: ":"},
				{Type: TokenColon, Value: ":"},
				{Type: TokenNumber, Value: "-1"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
		// null æ¯”è¾ƒ
		{
			`$[?@.foo == null]`,
			[]Token{
				{Type: TokenRoot, Value: "$"},
				{Type: TokenLBracket, Value: "["},
				{Type: TokenQuestion, Value: "?"},
				{Type: TokenCurrent, Value: "@"},
				{Type: TokenDot, Value: "."},
				{Type: TokenIdent, Value: "foo"},
				{Type: TokenEq, Value: "=="},
				{Type: TokenNull, Value: "null"},
				{Type: TokenRBracket, Value: "]"},
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			for i, expectToken := range tt.tokens {
				token := lexer.NextToken()
				if token.Type != expectToken.Type {
					t.Errorf("ä½ç½® %d: æœŸæœ›ç±»å‹ %v (%q), å®é™… %v (%q)",
						i, expectToken.Type, expectToken.Value, token.Type, token.Value)
				}
				if token.Value != expectToken.Value {
					t.Errorf("ä½ç½® %d: æœŸæœ›å€¼ %q, å®é™… %q", i, expectToken.Value, token.Value)
				}
			}
			// ç¡®ä¿æ²¡æœ‰å¤šä½™ token
			_eof := lexer.NextToken()
			if _eof.Type != TokenEOF {
				t.Errorf("æœŸæœ› EOF, å®é™… %v (%q)", _eof.Type, _eof.Value)
			}
		})
	}
}

// TestLexerEOF æµ‹è¯• EOF å¤„ç†
func TestLexerEOF(t *testing.T) {
	lexer := NewLexer("$")
	token := lexer.NextToken()
	if token.Type != TokenRoot {
		t.Errorf("ç¬¬ä¸€ä¸ª token åº”è¯¥æ˜¯ TokenRoot, å®é™… %v", token.Type)
	}

	for i := 0; i < 10; i++ {
		token = lexer.NextToken()
		if token.Type != TokenEOF {
			t.Errorf("ç¬¬ %d æ¬¡ NextToken() åº”è¯¥è¿”å› TokenEOF, å®é™… %v", i+1, token.Type)
		}
	}
}

// TestLexerIllegalTokens æµ‹è¯•éæ³• token
func TestLexerIllegalTokens(t *testing.T) {
	tests := []struct {
		input string
	}{
		{"="}, // å•ä¸ª = åº”è¯¥æ˜¯éæ³•çš„
		{"&"}, // å•ä¸ª & åº”è¯¥æ˜¯éæ³•çš„
		{"|"}, // å•ä¸ª | åº”è¯¥æ˜¯éæ³•çš„
		{"@"}, // @ åé¢æ²¡æœ‰å†…å®¹æ˜¯åˆæ³•çš„ï¼ˆå½“å‰èŠ‚ç‚¹æ ‡è¯†ç¬¦ï¼‰
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()

			// @ æ˜¯å•ç‹¬çš„åˆæ³• token
			if tt.input == "@" {
				if token.Type != TokenCurrent {
					t.Errorf("è¾“å…¥ %q: æœŸæœ› TokenCurrent, å®é™… %v", tt.input, token.Type)
				}
				return
			}

			// å…¶ä»–å•ä¸ªå­—ç¬¦åº”è¯¥æ˜¯éæ³•çš„
			if token.Type == TokenIllegal {
				// æ­£ç¡®
				return
			}
			// æˆ–è€…å®ƒä»¬è¢«è¯†åˆ«ä¸ºæŸä¸ªå…¶ä»– tokenï¼ˆè™½ç„¶ä¸æ˜¯æˆ‘ä»¬æœŸæœ›çš„ï¼‰
			t.Logf("è¾“å…¥ %q: è¢«è¯†åˆ«ä¸º %v (%q)", tt.input, token.Type, token.Value)
		})
	}
}

// TestLexerTokenPositions æµ‹è¯• token ä½ç½®ä¿¡æ¯
func TestLexerTokenPositions(t *testing.T) {
	input := "$ . name"
	lexer := NewLexer(input)

	expectedPositions := []int{0, 2, 4}
	expectedTypes := []TokenType{TokenRoot, TokenDot, TokenIdent}

	for i := 0; i < 3; i++ {
		token := lexer.NextToken()
		if token.Pos != expectedPositions[i] {
			t.Errorf("token %d: æœŸæœ›ä½ç½® %d, å®é™… %d", i, expectedPositions[i], token.Pos)
		}
		if token.Type != expectedTypes[i] {
			t.Errorf("token %d: æœŸæœ›ç±»å‹ %v, å®é™… %v", i, expectedTypes[i], token.Type)
		}
	}
}

// TestLexerStringsWithUnescapedQuotes RFC 9535 Section 2.3.1
// æµ‹è¯•å­—ç¬¦ä¸²ä¸­æœªè½¬ä¹‰çš„å¼•å·
func TestLexerStringsWithUnescapedQuotes(t *testing.T) {
	tests := []struct {
		input       string
		expectType  TokenType
		expectValue string
	}{
		// åŒå¼•å·å­—ç¬¦ä¸²ä¸­å¯ä»¥æœ‰å•å¼•å·
		{`"it's"`, TokenString, `it's`},
		// å•å¼•å·å­—ç¬¦ä¸²ä¸­å¯ä»¥æœ‰åŒå¼•å·
		{`'He said "hello"'`, TokenString, `He said "hello"`},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			if token.Type != tt.expectType {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›ç±»å‹ %v, å®é™… %v", tt.input, tt.expectType, token.Type)
			}
			if token.Value != tt.expectValue {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›å€¼ %q, å®é™… %q", tt.input, tt.expectValue, token.Value)
			}
		})
	}
}

// TestLexerArraySliceRFCExamples RFC 9535 Table 9
func TestLexerArraySliceRFCExamples(t *testing.T) {
	tests := []struct {
		input  string
		tokens []TokenType
	}{
		{"$[1:3]", []TokenType{TokenRoot, TokenLBracket, TokenNumber, TokenColon, TokenNumber, TokenRBracket}},
		{"$[5:]", []TokenType{TokenRoot, TokenLBracket, TokenNumber, TokenColon, TokenRBracket}},
		{"$[1:5:2]", []TokenType{TokenRoot, TokenLBracket, TokenNumber, TokenColon, TokenNumber, TokenColon, TokenNumber, TokenRBracket}},
		{"$[5:1:-2]", []TokenType{TokenRoot, TokenLBracket, TokenNumber, TokenColon, TokenNumber, TokenColon, TokenNumber, TokenRBracket}},
		{"$[::-1]", []TokenType{TokenRoot, TokenLBracket, TokenColon, TokenColon, TokenNumber, TokenRBracket}},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			for i, expectType := range tt.tokens {
				token := lexer.NextToken()
				if token.Type != expectType {
					t.Errorf("ä½ç½® %d: æœŸæœ›ç±»å‹ %v, å®é™… %v", i, expectType, token.Type)
				}
			}
		})
	}
}

// TestLexerFunctionNames RFC 9535 Section 2.4
// function-name-first = LCALPHA (a-z only)
// å‡½æ•°åå¿…é¡»ä»¥å°å†™å­—æ¯å¼€å¤´
func TestLexerFunctionNames(t *testing.T) {
	tests := []struct {
		input       string
		expectIdent bool // æœŸæœ›è¢«è¯†åˆ«ä¸ºæ ‡è¯†ç¬¦
	}{
		// RFC 9535 æ ‡å‡†å‡½æ•°
		{"length", true},
		{"count", true},
		{"match", true},
		{"search", true},
		{"value", true},

		// åˆæ³•çš„å‡½æ•°åï¼ˆå°å†™å¼€å¤´ï¼‰
		{"foo", true},
		{"bar123", true},
		{"my_func", true},

		// æ³¨æ„ï¼šå½“å‰ lexer å®ç°å…è®¸å¤§å†™å­—æ¯ä½œä¸ºæ ‡è¯†ç¬¦èµ·å§‹
		// ä½† RFC 9535 è§„å®šå‡½æ•°åå¿…é¡»ä»¥å°å†™å­—æ¯å¼€å¤´
		// è¿™æ˜¯è¯­æ³•å±‚é¢éœ€è¦æ£€æŸ¥çš„ï¼Œlexer åªè´Ÿè´£è¯†åˆ«æ ‡è¯†ç¬¦
		{"Length", true}, // è¯æ³•ä¸Šåˆæ³•ï¼Œä½†è¯­æ³•ä¸Šä¸æ˜¯åˆæ³•çš„å‡½æ•°å
		{"LENGTH", true}, // è¯æ³•ä¸Šåˆæ³•ï¼Œä½†è¯­æ³•ä¸Šä¸æ˜¯åˆæ³•çš„å‡½æ•°å
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			isIdent := token.Type == TokenIdent
			if isIdent != tt.expectIdent {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›æ ‡è¯†ç¬¦=%v, å®é™…=%v (ç±»å‹=%v)", tt.input, tt.expectIdent, isIdent, token.Type)
			}
			if isIdent && token.Value != tt.input {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›å€¼ %q, å®é™… %q", tt.input, tt.input, token.Value)
			}
		})
	}
}

// TestLexerMemberNameShorthand RFC 9535 Section 2.5.1
// member-name-shorthand = name-first *name-char
// name-first = ALPHA / "_" / %x80-D7FF / %xE000-10FFFF
func TestLexerMemberNameShorthand(t *testing.T) {
	tests := []struct {
		input       string
		expectIdent bool
	}{
		// ASCII å­—æ¯
		{"name", true},
		{"Name", true},
		{"_private", true},
		{"name123", true},

		// æ³¨æ„ï¼šæ•°å­—ä¸èƒ½ä½œä¸ºå¼€å¤´ï¼ˆä½†å¯ä»¥åœ¨åç»­ä½ç½®ï¼‰
		{"123name", false}, // ä¼šè¢«è¯†åˆ«ä¸ºæ•°å­—

		// ç‰¹æ®Šå­—ç¬¦ä¸èƒ½ä½œä¸ºæ ‡è¯†ç¬¦
		{"name-with-dash", false},
		{"name.with.dot", false},
	}

	for _, tt := range tests {
		t.Run(tt.input, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			token := lexer.NextToken()
			isIdent := token.Type == TokenIdent

			// å¯¹äº "name.with.dot"ï¼Œç¬¬ä¸€ä¸ª token åº”è¯¥æ˜¯ "name"
			if tt.input == "name.with.dot" {
				if token.Type != TokenIdent || token.Value != "name" {
					t.Errorf("è¾“å…¥ %q: ç¬¬ä¸€ä¸ª token åº”è¯¥æ˜¯æ ‡è¯†ç¬¦ 'name', å®é™… %v (%q)", tt.input, token.Type, token.Value)
				}
				return
			}

			// å¯¹äº "name-with-dash"ï¼Œç¬¬ä¸€ä¸ª token åº”è¯¥æ˜¯ "name"
			if tt.input == "name-with-dash" {
				if token.Type != TokenIdent || token.Value != "name" {
					t.Errorf("è¾“å…¥ %q: ç¬¬ä¸€ä¸ª token åº”è¯¥æ˜¯æ ‡è¯†ç¬¦ 'name', å®é™… %v (%q)", tt.input, token.Type, token.Value)
				}
				return
			}

			if isIdent != tt.expectIdent {
				t.Errorf("è¾“å…¥ %q: æœŸæœ›æ ‡è¯†ç¬¦=%v, å®é™…=%v (ç±»å‹=%v)", tt.input, tt.expectIdent, isIdent, token.Type)
			}
		})
	}
}

// TestLexerRFCExamples RFC 9535 ä¸­çš„ç¤ºä¾‹è¡¨è¾¾å¼è¯æ³•åˆ†æ
func TestLexerRFCExamples(t *testing.T) {
	// RFC 9535 Table 2: ç¤ºä¾‹ JSONPath è¡¨è¾¾å¼
	examples := []string{
		`$.store.book[*].author`,
		`$..author`,
		`$.store.*`,
		`$.store..price`,
		`$..book[2]`,
		`$..book[2].author`,
		`$..book[2].publisher`,
		`$..book[-1]`,
		`$..book[0,1]`,
		`$..book[:2]`,
		`$..book[?@.isbn]`,
		`$..book[?@.price<10]`,
		`$..*`,
	}

	for _, example := range examples {
		t.Run(example, func(t *testing.T) {
			lexer := NewLexer(example)
			tokenCount := 0
			hasIllegal := false

			for {
				token := lexer.NextToken()
				if token.Type == TokenEOF {
					break
				}
				if token.Type == TokenIllegal {
					t.Errorf("ç¤ºä¾‹ %q åŒ…å«éæ³• token: %q", example, token.Value)
					hasIllegal = true
					break
				}
				tokenCount++
			}

			if !hasIllegal && tokenCount == 0 {
				t.Errorf("ç¤ºä¾‹ %q æ²¡æœ‰äº§ç”Ÿä»»ä½• token", example)
			}
		})
	}
}

// BenchmarkLexerSimple ç®€å•è¡¨è¾¾å¼åŸºå‡†æµ‹è¯•
func BenchmarkLexerSimple(b *testing.B) {
	input := "$.store.book[0].title"
	for i := 0; i < b.N; i++ {
		lexer := NewLexer(input)
		for lexer.NextToken().Type != TokenEOF {
		}
	}
}

// BenchmarkLexerComplex å¤æ‚è¡¨è¾¾å¼åŸºå‡†æµ‹è¯•
func BenchmarkLexerComplex(b *testing.B) {
	input := `$.store.book[?@.price < 10 && @.category == 'fiction'].title`
	for i := 0; i < b.N; i++ {
		lexer := NewLexer(input)
		for lexer.NextToken().Type != TokenEOF {
		}
	}
}

// BenchmarkLexerWithUnicode Unicode å­—ç¬¦ä¸²åŸºå‡†æµ‹è¯•
func BenchmarkLexerWithUnicode(b *testing.B) {
	input := "$[?@.name == 'ä¸­æ–‡æµ‹è¯•']"
	for i := 0; i < b.N; i++ {
		lexer := NewLexer(input)
		for lexer.NextToken().Type != TokenEOF {
		}
	}
}
